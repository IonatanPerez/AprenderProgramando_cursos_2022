{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "unauthorized-blackjack",
   "metadata": {},
   "source": [
    "Para poder visualizar los árboles de decisión que entrenaremos necesitamos instalar una librearía específica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corporate-thousand",
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install python-graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "french-yahoo",
   "metadata": {},
   "source": [
    "# Overfitting y Underfitting\n",
    "\n",
    "Como vimos, un modelo sobreajustado es un modelo que es tan específico en su entrenamiento que analiza a la perfeccción los datos conocidos pero al darle datos nuevos no lográ hacer una buena predicción. El caso de un modelo subajustado es el contrario, cuando es demasiado genérico en su predicción y no logra los resultados esperados. \n",
    "\n",
    "En esta Notebook veremos un ejemplo de un modelo sobreajustado y forma de resolverlo. El ejemplo que utilizaremos será un Árbol de decisión que es un modelo que tiende al sobreajuste.\n",
    "\n",
    "Vamos a realizar los pasos de un Proyecto de Ciencia de datos e intentaremos mejorar su performance. \n",
    "\n",
    "1. Definición del Problema\n",
    "2. Búsqueda de datos \n",
    "3. Exploración y Limpieza de Datos\n",
    "4. Dividir los datos en **X** (variables predictoras) e **y** (variable a predecir). Dividir los datos en entrenamiento y testo con el méodo *train_test_split*\n",
    "5. Entrenamiento del modelo\n",
    "6. Testeo del Modelo \n",
    "\n",
    "### Definición del problema\n",
    "\n",
    "**¿Es peligroso un asteroide cercano a la Tierra en base a determinadas características?**\n",
    "\n",
    "### Búsqueda de datos\n",
    "\n",
    "El dataset que se utilizará es de la Nasa y muestra distintos asteroides cercanos a la Tierra y sus características para poder predecir su peligrosidad *hazardous (peligroso)*. Fue descargado de [Kaggle](https://www.kaggle.com/datasets/sameepvani/nasa-nearest-earth-objects)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formal-surge",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importamos las librerías que utilizaremos\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uniform-shift",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"neo.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaging-devices",
   "metadata": {},
   "source": [
    "##### Exploración y limpieza del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "first-indonesian",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retired-remark",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlled-deficit",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"orbiting_body\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blond-diabetes",
   "metadata": {},
   "source": [
    "##### Limpieza del dataset\n",
    "\n",
    "Eliminamos las variables *id* y *name* ya que no servirán para el entrenamiento del modelo. \n",
    "Luego de realizado esto no habrá variables categóricas (salvo la variable a predecir).\n",
    "También eliminamos la variable *orbiting_body* ya que en todos los casos recibe el mismo valor por lo cual tampoco es útil para el entrenamiento del modelo. \n",
    "\n",
    "Luego observamos la distribución de la variable target *hazardous*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daily-being",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos las variables que no serán útiles para el entrenamiento del modelo\n",
    "\n",
    "data.drop(columns= [\"id\", \"name\", \"orbiting_body\"], inplace=True)\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "together-audit",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distribución de la variable target\n",
    "\n",
    "data[\"hazardous\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "located-cambodia",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data[\"hazardous\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contemporary-greeting",
   "metadata": {},
   "source": [
    "#### Entrenamiento del modelo\n",
    "\n",
    "Vamos a entrenar un modelo de Árbol de decisión para la peligrosidad de un asteroide. En primer lugar dividermos los datos en **X** e **y** y en entrenamiento y testo.\n",
    "\n",
    "Instanciaremos el modelo de Árbol de decisión con los hiperparámentros por default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compatible-designer",
   "metadata": {
    "id": "preliminary-comparative"
   },
   "outputs": [],
   "source": [
    "# Generamos X e y \n",
    "\n",
    "X = data.drop(columns = \"hazardous\")   #variables predictora\n",
    "y = data[\"hazardous\"]   #variable a predecir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tender-bidder",
   "metadata": {
    "id": "finished-scale"
   },
   "outputs": [],
   "source": [
    "# Dividimos datos en train y test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=138)  #por default 25% de test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recent-heart",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos el modelo que utilizaremos\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overall-efficiency",
   "metadata": {
    "id": "japanese-gossip"
   },
   "outputs": [],
   "source": [
    "#Instanciamos el modelo que utilizaremos\n",
    "\n",
    "arbol = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inclusive-monaco",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "satisfactory-capability",
    "outputId": "220644a0-f766-4f2b-bd02-959df46ebb9d"
   },
   "outputs": [],
   "source": [
    "#Entrenamos el modelo \n",
    "\n",
    "arbol.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extended-panic",
   "metadata": {},
   "source": [
    "##### Visualización del Árbol generado por el modelo\n",
    "\n",
    "Dada la cantidad de valores que tiene nuestro dataset y lo específico del arbol, al intentar graficarlo vamos a tener problemas de performance en nuestra notebook. \n",
    "\n",
    "Podemos conocer la cantidad de nodos que tiene nuestro Árbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occasional-incidence",
   "metadata": {},
   "outputs": [],
   "source": [
    "arbol.tree_.node_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surrounded-centre",
   "metadata": {},
   "outputs": [],
   "source": [
    "arbol.tree_.max_depth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elder-consciousness",
   "metadata": {},
   "source": [
    "##### Evaluación de datos de train\n",
    "\n",
    "En este caso, además de utilizar las métricas para evaluar los datos de testo, también utilizaremos las métricas para evaluar cómo el modelo fue entrenado, es decir, si utilizamos el método predict con los datos de entrenamiento y comparamos con los datos de entrenamiento. \n",
    "\n",
    "Esto no sirve para saber la performance del modelo ya que fue entrenado con los mismo datos, pero sirve para analizar como fue entrenado el modelo y, en el caso de los árboles de decisión, la exactitud con la que los datos se organizan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exciting-volunteer",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred_train = arbol.predict(X_train)\n",
    "exactitud_train_arbol = accuracy_score(y_train, y_pred_train)\n",
    "exactitud_train_arbol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spoken-thompson",
   "metadata": {},
   "source": [
    "Podemos observar que el resultado de evaluar los datos de entrenamiento es perfecta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "figured-saturday",
   "metadata": {},
   "source": [
    "##### Probamos y evaluamos nuestro modelo\n",
    "\n",
    "Utilizamos el metodo *predict* para probar nuestro modelo con los datos de test. Luego comparamos la predicciones de nuestro modelo con el resultado real a través de una matrix de confusión y utilizando la métrica accuracy (exactitud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "toxic-operator",
   "metadata": {
    "id": "agreed-library"
   },
   "outputs": [],
   "source": [
    "# Probamos nuestro modelo con los datos de test\n",
    "\n",
    "y_pred_arbol = arbol.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "productive-visibility",
   "metadata": {
    "id": "informal-intersection"
   },
   "outputs": [],
   "source": [
    "# Matriz de confusión:comparando resultado original (y_test) con predicción del modelo (y_pred_arbol_completo)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "matriz_arbol = confusion_matrix(y_test, y_pred_arbol)\n",
    "sns.heatmap(matriz_arbol, annot=True, fmt = \"d\")\n",
    "plt.xlabel(\"Etiquetas predichas\")\n",
    "plt.ylabel(\"Etiquetas reales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confidential-steal",
   "metadata": {
    "id": "rotary-hardware"
   },
   "outputs": [],
   "source": [
    "#metrica accuracy\n",
    "\n",
    "exactitud_arbol = accuracy_score(y_test, y_pred_arbol)\n",
    "exactitud_arbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "completed-munich",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"La exactitud del modelo árbol de decisión es\", round(exactitud_arbol,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "korean-currency",
   "metadata": {},
   "source": [
    "#### Hiperparámetro distintos\n",
    "\n",
    "Como hablamso los árboles de decisión, estos tienen una tendencia al sobreajuste (overfitting). \n",
    "Para evitar eso es posible ajustar los distintos hiperpárametros para reducir la complejidad de los árboles, se utiliza un mecanismo denominado \"poda\", reduciendo el tamaño del árbol a través de limitar la profundidad máxima, limitar el número de muestrar requeridas por cada hoja o limitando el número mínimo de muestras para particionar. Se puede ver en la [documentación](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html?highlight=tree#sklearn.tree.DecisionTreeClassifier)\n",
    "\n",
    "Algunos de los hiperparámetros que se pueden ajustar en el modelo de Scikit-Lear son:.\n",
    "\n",
    "- **max_depth:** profundidad máxima del árbol. Solemos determinar una profundidad máxima para evitar que el modelo sobreajuste.  \n",
    "- **min_samples_split:** número mínimo de muestras que un nodo debe contener para considerar la división. El valor predeterminado es dos. Podemos usar este parámetro para regularizar el árbol.\n",
    "- **min_samples_leaf:** número mínimo de muestras necesarias para ser considerado un nodo hoja. El valor predeterminado se establece en uno. Este parámetro se utiliza como una forma alternativa de limitar el crecimiento del árbol.  \n",
    "- **max_features:** número de características a considerar al buscar la mejor división. Si no se establece este valor, el árbol de decisión considerará todas las variables independientes disponibles para hacer la mejor división.\n",
    "\n",
    "En este caso vamos a entrenar y evaluar dos modelos modificando el hiperparámetro: \"max_depth\" y observando la diferencia a través de las visualizaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "employed-memphis",
   "metadata": {
    "id": "japanese-gossip"
   },
   "outputs": [],
   "source": [
    "# Instanciamos el modelo que utilizaremos\n",
    "\n",
    "arbol_depth4 = DecisionTreeClassifier(max_depth=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empirical-rapid",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "satisfactory-capability",
    "outputId": "220644a0-f766-4f2b-bd02-959df46ebb9d"
   },
   "outputs": [],
   "source": [
    "# Entrenamos el modelo \n",
    "\n",
    "arbol_depth4.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empty-amsterdam",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "class_names = ['True', 'False']\n",
    "plt.figure(figsize = (20,20))\n",
    "tree.plot_tree(arbol_depth4, feature_names=data.columns[:-1],filled=True,rounded=True, class_names=class_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daily-payroll",
   "metadata": {},
   "source": [
    "##### Evaluación de datos de train\n",
    "\n",
    "Podemos utilizar una métrica de evaluación para ver la performance del modelo con los datos de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "listed-sheffield",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train_depth4 = arbol_depth4.predict(X_train)\n",
    "exactitud_train_depth4 = accuracy_score(y_train, y_pred_train_depth4)\n",
    "exactitud_train_depth4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liquid-plymouth",
   "metadata": {},
   "source": [
    "##### Probamos y evaluamos nuestro modelo\n",
    "\n",
    "Utilizamos el metodo predict para probar nuestro modelo con los datos de test. Luego comparamos la predicciones de nuestro modelo con el resultado real a través de una matrix de confusión y utilizando la métrica accuracy (exactitud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monetary-cambodia",
   "metadata": {
    "id": "agreed-library"
   },
   "outputs": [],
   "source": [
    "# Probar nuestro modelo con los datos de test\n",
    "\n",
    "y_pred_depth4 = arbol_depth4.predict(X_test)\n",
    "y_pred_depth4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heard-patrick",
   "metadata": {
    "id": "informal-intersection"
   },
   "outputs": [],
   "source": [
    "# Matriz de confusión:comparando resultado original (y_test) con predicción del modelo (y_pred_depth4)\n",
    "matriz_depth4 = confusion_matrix(y_test, y_pred_depth4)\n",
    "sns.heatmap(matriz_depth4, annot=True, fmt=\"d\")\n",
    "plt.xlabel(\"Etiquetas predichas\")\n",
    "plt.ylabel(\"Etiquetas reales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sacred-pierre",
   "metadata": {
    "id": "rotary-hardware"
   },
   "outputs": [],
   "source": [
    "exactitud_depth4 = accuracy_score(y_test, y_pred_depth4)\n",
    "exactitud_depth4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specified-convert",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"La exactitud del modelo con los hiperparámetros por default es\", round(exactitud_arbol,2))\n",
    "print(\"La exactitud del modelo hiperparámetro profundidad máxima de 4 es\", round(exactitud_depth4,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ready-vienna",
   "metadata": {},
   "source": [
    "#### Entrenamos nuestro modelo con otro hiperparámetro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annual-episode",
   "metadata": {
    "id": "japanese-gossip"
   },
   "outputs": [],
   "source": [
    "# Instanciamos el modelo que utilizaremos\n",
    "\n",
    "arbol_depth2 = DecisionTreeClassifier(max_depth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abandoned-vatican",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "satisfactory-capability",
    "outputId": "220644a0-f766-4f2b-bd02-959df46ebb9d"
   },
   "outputs": [],
   "source": [
    "# Entrenamos el modelo \n",
    "\n",
    "arbol_depth2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conscious-bridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['True', 'False']\n",
    "plt.figure(figsize = (10,10))\n",
    "tree.plot_tree(arbol_depth2, feature_names=data.columns[:-1],filled=True,rounded=True, class_names=class_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "active-powder",
   "metadata": {},
   "source": [
    "##### Evaluación de datos de train\n",
    "\n",
    "Podemos utilizar una métrica de evaluación para ver la performance del modelo con los datos de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standing-spread",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train_depth2 = arbol_depth2.predict(X_train)\n",
    "exactitud_train_depth2 = accuracy_score(y_train, y_pred_train_depth2)\n",
    "exactitud_train_depth2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behavioral-board",
   "metadata": {},
   "source": [
    "##### Probamos y evaluamos nuestro modelo\n",
    "\n",
    "Utilizamos el metodo predict para probar nuestro modelo con los datos de test. Luego comparamos la predicciones de nuestro modelo con el resultado real a través de una matrix de confusión y utilizando la métrica accuracy (exactitud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thick-sodium",
   "metadata": {
    "id": "agreed-library"
   },
   "outputs": [],
   "source": [
    "# Probar nuestro modelo con los datos de test\n",
    "\n",
    "y_pred_depth2 = arbol_depth2.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respective-marine",
   "metadata": {
    "id": "informal-intersection"
   },
   "outputs": [],
   "source": [
    "# Matriz de confusión:comparando resultado original (y_test) con predicción del modelo (y_pred_depth2)\n",
    "matriz_depth2 = confusion_matrix(y_test, y_pred_depth2)\n",
    "sns.heatmap(matriz_depth2, annot=True, fmt=\"d\")\n",
    "plt.xlabel(\"Etiquetas predichas\")\n",
    "plt.ylabel(\"Etiquetas reales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saving-worse",
   "metadata": {
    "id": "rotary-hardware"
   },
   "outputs": [],
   "source": [
    "#accuracy\n",
    "\n",
    "exactitud_depth2 = accuracy_score(y_test, y_pred_depth2)\n",
    "exactitud_depth2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "progressive-change",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparamos todos los modelos\n",
    "\n",
    "print(\"La exactitud del modelo con los hiperparámetros por default es\", round(exactitud_arbol,2))\n",
    "print(\"La exactitud del modelo con hiperparámetro profundidad máxima de 4 es\", round(exactitud_depth4,2))\n",
    "print(\"La exactitud del modelo con hiperparámetro profundidad máxima de 2 es\", round(exactitud_depth2,2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affected-cookbook",
   "metadata": {},
   "source": [
    "### Conclusión\n",
    "\n",
    "El Árbol de decisión entrenado tiende al sobreajuste. Esto significa que es tan correcto el entrenamiento (la performance en los datos de entrenamiento) pero puede ser malo al darle datos nuevo como los datos de testeo (y en la realidad), ya que es demasiado específico para los datos con los que fue entrenado.\n",
    "\n",
    "En este ejemplo vimos como \"recortando\" el árbol y simplificandolo empeoró la performance para los datos de entrenamiento pero mejoró para los datos de testeo. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
